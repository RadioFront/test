{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "090347b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time, sys\n",
    "from pymavlink import mavutil\n",
    "import threading\n",
    "import math\n",
    "\n",
    "#import base64\n",
    "#import imageio\n",
    "#import IPython\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "#import PIL.Image\n",
    "#import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad322efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# parameters\n",
    "target_elevation = 50\n",
    "period_action = 0.1\n",
    "period_telemetry = 0.1\n",
    "frequency_message = 10\n",
    "period_set_position = 1/5\n",
    "time_arm_mode = 1\n",
    "time_settling = 10\n",
    "stamp_state = time.time()\n",
    "stamp_telemetry = time.time()\n",
    "stamp_set_position = time.time()\n",
    "period_delay = 0.01\n",
    "time_steady_state = 1\n",
    "#no_sample=1200\n",
    "sample_period = 0.1\n",
    "mode = \"none\"\n",
    "state = \"arm\"\n",
    "stampState=time.time()\n",
    "stampMode=time.time()\n",
    "vec_telemetry=[]\n",
    "vec_pid_update=[]\n",
    "#####################\n",
    "# drone controller parameters\n",
    "## Rate Controller\n",
    "MC_ROLLRATE_K = 1.0 #0.3 - 3\n",
    "MC_ROLLRATE_D = 0.003 #0.0004 - 0.01\n",
    "MC_ROLLRATE_I = 0.2 #0.1 - 0.5\n",
    "MC_PITCHRATE_K = 1.0 #0.3 - 3\n",
    "MC_PITCHRATE_D = 0.003 #0.0004 - 0.01\n",
    "MC_PITCHRATE_I = 0.2 #0.1 - 0.5\n",
    "MC_YAWRATE_K = 1.0 #0.3 - 3\n",
    "MC_YAWRATE_I = 0.1 #0.04 - 0.4\n",
    "## Attitude Controller\n",
    "MC_ROLL_P = 6.5 #1-14\n",
    "MC_PITCH_P = 6.5 #1-14\n",
    "MC_YAW_P = 2.8 #1-5\n",
    "## Velocity Controller\n",
    "MPC_XY_VEL_P_ACC = 1.8 #1.2-5\n",
    "MPC_XY_VEL_I_ACC = 0.2 #0.2-10\n",
    "MPC_XY_VEL_D_ACC = 0.2 #0.1-2\n",
    "MPC_Z_VEL_P_ACC = 4.0 #2-15\n",
    "MPC_Z_VEL_I_ACC = 2.0 #0.2-3\n",
    "MPC_Z_VEL_D_ACC = 0.0 #0-2\n",
    "## Position Controller\n",
    "MPC_XY_P = 0.95 #0-2\n",
    "MPC_Z_P = 1.0 #0-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d93e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def px4_arm():\n",
    "    # Arm motors\n",
    "    state = \"arm\"\n",
    "    print(\"State:\",state)\n",
    "    master.mav.command_long_send(\n",
    "        master.target_system, master.target_component,\n",
    "        mavutil.mavlink.MAV_CMD_COMPONENT_ARM_DISARM,\n",
    "        0,\n",
    "        1, 0, 0, 0, 0, 0, 0)\n",
    "    master.motors_armed_wait()\n",
    "    state = \"set_mode\"\n",
    "    print(\"State:\",state)\n",
    "    stampState = time.time()\n",
    "    stampMode = time.time()\n",
    "\n",
    "def px4_steady_state():\n",
    "    ##### Put down at steady state\n",
    "    mode = \"none\"\n",
    "    state = \"set_mode\"\n",
    "    stampState = time.time()\n",
    "    stamp_set_position = time.time()\n",
    "    while mode!=\"steady_state\":\n",
    "        # set OFFBOARD mode\n",
    "        if state == \"set_mode\" and ( time.time() - stampState > time_arm_mode ):\n",
    "            master.mav.command_long_send(\n",
    "                master.target_system,\n",
    "                master.target_component,\n",
    "                mavutil.mavlink.MAV_CMD_DO_SET_MODE,\n",
    "                0,\n",
    "                209, 6, 0, 0, 0, 0, 0) #OFFBOARD mode\n",
    "            state = \"mode_wait\"\n",
    "            print(\"State:\",state)\n",
    "            stampState = time.time()\n",
    "\n",
    "        #wait fof OFFBOARD mode acknowledged\n",
    "        if state == \"mode_wait\" and ( time.time() - stampState > time_arm_mode):\n",
    "            ack_msg = master.recv_match(type='COMMAND_ACK')\n",
    "            if ack_msg:\n",
    "                ack_msg = ack_msg.to_dict()\n",
    "                if ack_msg['command'] == mavutil.mavlink.MAV_CMD_DO_SET_MODE:\n",
    "                    print(mavutil.mavlink.enums['MAV_RESULT'][ack_msg['result']].description)\n",
    "                    state = \"ramp_up\"\n",
    "                    stampState = time.time()\n",
    "                    print(\"State:\",state)\n",
    "                    \n",
    "        # wait till the drone reaches target position\n",
    "        if state == \"ramp_up\" and ( time.time() - stampState > time_settling):\n",
    "            state = \"flight_mode\"\n",
    "            print(\"State:\",state)\n",
    "            stampState = time.time()\n",
    "\n",
    "        # repeat position setup > 2 Hz, to avoid fail safe mode kick in\n",
    "        if time.time() - stamp_set_position > period_set_position:\n",
    "            stamp_set_position = time.time()\n",
    "            px4_set_target_depth_local(-target_elevation)\n",
    "\n",
    "        # unit delay\n",
    "        time.sleep(period_delay)\n",
    "\n",
    "        t_param='POSITION_TARGET_LOCAL_NED' #85\n",
    "        try: \n",
    "            POSITION_TARGET_LOCAL_NED_x= master.messages[t_param].x\n",
    "            #POSITION_TARGET_LOCAL_NED_y= master.messages[t_param].y\n",
    "            #POSITION_TARGET_LOCAL_NED_z= master.messages[t_param].z\n",
    "            #print(t_param, \":\", POSITION_TARGET_LOCAL_NED_x,POSITION_TARGET_LOCAL_NED_y,POSITION_TARGET_LOCAL_NED_z)\n",
    "            if mode == \"none\" and math.isnan(POSITION_TARGET_LOCAL_NED_x) == False:\n",
    "                mode = \"flight_mode\"\n",
    "                stampMode = time.time()\n",
    "                print(\"Mode:\",mode)\n",
    "        except:\n",
    "            print(t_param,\":\",'No message received')\n",
    "\n",
    "        t_param='LOCAL_POSITION_NED' #32\n",
    "        try: \n",
    "            LOCAL_POSITION_NED_x = master.messages[t_param].x\n",
    "            LOCAL_POSITION_NED_y = master.messages[t_param].y\n",
    "            LOCAL_POSITION_NED_z = master.messages[t_param].z\n",
    "            #print(t_param, \":\", LOCAL_POSITION_NED_x, LOCAL_POSITION_NED_y, LOCAL_POSITION_NED_z)\n",
    "            if mode == \"flight_mode\" and distance_target([LOCAL_POSITION_NED_x, LOCAL_POSITION_NED_y, LOCAL_POSITION_NED_z])<0.3:\n",
    "                mode = \"steady_state\"\n",
    "                stampMode = time.time()\n",
    "                print(\"Mode:\",mode)\n",
    "        except:\n",
    "            print(t_param, \":\", 'No message received')\n",
    "\n",
    "def distance_geo(A,B):\n",
    "    return math.sqrt( (A[0]-B[0])**2 + (A[1]-B[1])**2 + (A[2]-B[2])**2 )\n",
    "\n",
    "def distance_target(A):\n",
    "    return math.sqrt( A[0]**2 + A[1]**2 + (A[2]+target_elevation)**2 )\n",
    "\n",
    "def px4_request_message_interval(message_id: int, frequency_hz: float):\n",
    "    master.mav.command_long_send(\n",
    "        master.target_system, master.target_component,\n",
    "        mavutil.mavlink.MAV_CMD_SET_MESSAGE_INTERVAL, 0,\n",
    "        message_id, # The MAVLink message ID\n",
    "        1e6 / frequency_hz, # The interval between two messages in microseconds. Set to -1 to disable and 0 to request default rate.\n",
    "        0, 0, 0, 0, # Unused parameters\n",
    "        0, # Target address of message stream (if message has target address fields). 0: Flight-stack default (recommended), 1: address of requestor, 2: broadcast.\n",
    "    )\n",
    "\n",
    "def px4_set_target_depth_local(depth):\n",
    "    master.mav.set_position_target_local_ned_send( #84\n",
    "        int(1e3 * (time.time() - boot_time)), # ms since boot\n",
    "        master.target_system, master.target_component,\n",
    "        coordinate_frame=mavutil.mavlink.MAV_FRAME_LOCAL_NED,\n",
    "        type_mask=( # ignore everything except z position\n",
    "            # mavutil.mavlink.POSITION_TARGET_TYPEMASK_X_IGNORE |\n",
    "            # mavutil.mavlink.POSITION_TARGET_TYPEMASK_Y_IGNORE |\n",
    "            # mavutil.mavlink.POSITION_TARGET_TYPEMASK_Z_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_VX_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_VY_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_VZ_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_AX_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_AY_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_AZ_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_FORCE_SET |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_YAW_IGNORE |\n",
    "            mavutil.mavlink.POSITION_TARGET_TYPEMASK_YAW_RATE_IGNORE\n",
    "        ), x=0, y=0, z=depth,\n",
    "        vx=0, vy=0, vz=0, \n",
    "        afx=0, afy=0, afz=0, yaw=0, yaw_rate=0\n",
    "    )\n",
    "\n",
    "def px4_request_message():\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_SYS_STATUS, frequency_message) #1\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_SET_MODE, frequency_message) #1\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_GPS_RAW_INT, frequency_message) #24\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_SCALED_IMU, frequency_message) #26\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_ATTITUDE, frequency_message) #30\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_ATTITUDE_QUATERNION, frequency_message) #31\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_LOCAL_POSITION_NED, frequency_message) #32\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_GLOBAL_POSITION_INT, frequency_message) #33\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_SERVO_OUTPUT_RAW, frequency_message) #36\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_VFR_HUD, frequency_message) #74\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_ATTITUDE_TARGET, frequency_message) #83\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_POSITION_TARGET_LOCAL_NED, frequency_message) #85\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_POSITION_TARGET_GLOBAL_INT, frequency_message) #87\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_ALTITUDE, frequency_message) #141\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_BATTERY_STATUS, frequency_message) #147\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_ESTIMATOR_STATUS, frequency_message) #230\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_VIBRATION, frequency_message) #241\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_UTM_GLOBAL_POSITION, frequency_message) #340\n",
    "    px4_request_message_interval(mavutil.mavlink.MAVLINK_MSG_ID_OPEN_DRONE_ID_LOCATION, frequency_message) #12901 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00f4dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def px4_reset_pid_controller():\n",
    "    #####################\n",
    "    # drone controller parameters\n",
    "    ## Rate Controller\n",
    "    MC_ROLLRATE_K = 1.0 #0.3 - 3\n",
    "    MC_ROLLRATE_D = 0.003 #0.0004 - 0.01\n",
    "    MC_ROLLRATE_I = 0.2 #0.1 - 0.5\n",
    "    MC_PITCHRATE_K = 1.0 #0.3 - 3\n",
    "    MC_PITCHRATE_D = 0.003 #0.0004 - 0.01\n",
    "    MC_PITCHRATE_I = 0.2 #0.1 - 0.5\n",
    "    MC_YAWRATE_K = 1.0 #0.3 - 3\n",
    "    MC_YAWRATE_I = 0.1 #0.04 - 0.4\n",
    "    ## Attitude Controller\n",
    "    MC_ROLL_P = 6.5 #1-14\n",
    "    MC_PITCH_P = 6.5 #1-14\n",
    "    MC_YAW_P = 2.8 #1-5\n",
    "    ## Velocity Controller\n",
    "    MPC_XY_VEL_P_ACC = 1.8 #1.2-5\n",
    "    MPC_XY_VEL_I_ACC = 0.2 #0.2-10\n",
    "    MPC_XY_VEL_D_ACC = 0.2 #0.1-2\n",
    "    MPC_Z_VEL_P_ACC = 4.0 #2-15\n",
    "    MPC_Z_VEL_I_ACC = 2.0 #0.2-3\n",
    "    MPC_Z_VEL_D_ACC = 0.0 #0-2\n",
    "    ## Position Controller\n",
    "    MPC_XY_P = 0.95 #0-2\n",
    "    MPC_Z_P = 1.0 #0-2\n",
    "    #####################\n",
    "    ## Rate Controller\n",
    "    #PITCH\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_K', MC_PITCHRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_D', MC_PITCHRATE_D, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_I', MC_PITCHRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #ROLL\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_K', MC_ROLLRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_D', MC_ROLLRATE_D, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_I', MC_ROLLRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #YAW\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAWRATE_K', MC_YAWRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAWRATE_I', MC_YAWRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Attitude Controller\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLL_P', MC_ROLL_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCH_P', MC_PITCH_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAW_P', MC_YAW_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Velocity Controller\n",
    "    #XY\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_P_ACC', MPC_XY_VEL_P_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_I_ACC', MPC_XY_VEL_I_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_D_ACC', MPC_XY_VEL_D_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #Z\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_P_ACC', MPC_Z_VEL_P_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_I_ACC', MPC_Z_VEL_I_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_D_ACC', MPC_Z_VEL_D_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Position Controller\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_P', MPC_XY_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_P', MPC_Z_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "\n",
    "def px4_update_pid_controller_rate_att(action):\n",
    "    ## Rate Controller\n",
    "    MC_PITCHRATE_K=action[0]\n",
    "    MC_PITCHRATE_D=action[1]\n",
    "    MC_PITCHRATE_I=action[2]\n",
    "    MC_ROLLRATE_K=action[3]\n",
    "    MC_ROLLRATE_D=action[4]\n",
    "    MC_ROLLRATE_I=action[5]\n",
    "    MC_YAWRATE_K=action[6]\n",
    "    MC_YAWRATE_I=action[7]\n",
    "    #PITCH\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_K', MC_PITCHRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_D', MC_PITCHRATE_D, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_I', MC_PITCHRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #ROLL\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_K', MC_ROLLRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_D', MC_ROLLRATE_D, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_I', MC_ROLLRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #YAW\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAWRATE_K', MC_YAWRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAWRATE_I', MC_YAWRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "\n",
    "def px4_update_pid_controller_velocity_pos(action):\n",
    "    ## Velocity Controller\n",
    "    MPC_XY_VEL_P_ACC = action[0] #1.8 #1.2-5\n",
    "    MPC_XY_VEL_I_ACC = action[1] #0.2 #0.2-10\n",
    "    MPC_XY_VEL_D_ACC = action[2] #0.2 #0.1-2\n",
    "    MPC_Z_VEL_P_ACC = action[3] #4.0 #2-15\n",
    "    MPC_Z_VEL_I_ACC = action[4] #2.0 #0.2-3\n",
    "    MPC_Z_VEL_D_ACC = action[5] #0.0 #0-2\n",
    "    ## Position Controller\n",
    "    MPC_XY_P = action[6] #0.95 #0-2\n",
    "    MPC_Z_P = action[7] #1.0 #0-2\n",
    "    ## Velocity Controller\n",
    "    #XY\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_P_ACC', MPC_XY_VEL_P_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_I_ACC', MPC_XY_VEL_I_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_D_ACC', MPC_XY_VEL_D_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #Z\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_P_ACC', MPC_Z_VEL_P_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_I_ACC', MPC_Z_VEL_I_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_D_ACC', MPC_Z_VEL_D_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Position Controller\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_P', MPC_XY_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_P', MPC_Z_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "\n",
    "def px4_update_pid_controller_all(action):\n",
    "    ## Rate Controller\n",
    "    MC_PITCHRATE_K=action[0]\n",
    "    MC_PITCHRATE_D=action[1]\n",
    "    MC_PITCHRATE_I=action[2]\n",
    "    MC_ROLLRATE_K=action[3]\n",
    "    MC_ROLLRATE_D=action[4]\n",
    "    MC_ROLLRATE_I=action[5]\n",
    "    MC_YAWRATE_K=action[6]\n",
    "    MC_YAWRATE_I=action[7]\n",
    "    ## Attitude Controller\n",
    "    MC_ROLL_P = action[8] #6.5 #1-14\n",
    "    MC_PITCH_P = action[9] #6.5 #1-14\n",
    "    MC_YAW_P = action[10] #2.8 #1-5    ## Velocity Controller\n",
    "    MPC_XY_VEL_P_ACC = action[11] #1.8 #1.2-5\n",
    "    MPC_XY_VEL_I_ACC = action[12] #0.2 #0.2-10\n",
    "    MPC_XY_VEL_D_ACC = action[13] #0.2 #0.1-2\n",
    "    MPC_Z_VEL_P_ACC = action[14] #4.0 #2-15\n",
    "    MPC_Z_VEL_I_ACC = action[15] #2.0 #0.2-3\n",
    "    MPC_Z_VEL_D_ACC = action[16] #0.0 #0-2\n",
    "    ## Position Controller\n",
    "    MPC_XY_P = action[17] #0.95 #0-2\n",
    "    MPC_Z_P = action[18] #1.0 #0-2\n",
    "    #PITCH\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_K', MC_PITCHRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_D', MC_PITCHRATE_D, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCHRATE_I', MC_PITCHRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #ROLL\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_K', MC_ROLLRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_D', MC_ROLLRATE_D, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLLRATE_I', MC_ROLLRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #YAW\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAWRATE_K', MC_YAWRATE_K, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAWRATE_I', MC_YAWRATE_I, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Attitude Controller\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_ROLL_P', MC_ROLL_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_PITCH_P', MC_PITCH_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MC_YAW_P', MC_YAW_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Velocity Controller\n",
    "    #XY\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_P_ACC', MPC_XY_VEL_P_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_I_ACC', MPC_XY_VEL_I_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_VEL_D_ACC', MPC_XY_VEL_D_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    #Z\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_P_ACC', MPC_Z_VEL_P_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_I_ACC', MPC_Z_VEL_I_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_VEL_D_ACC', MPC_Z_VEL_D_ACC, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    ## Position Controller\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_XY_P', MPC_XY_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)\n",
    "    master.mav.param_set_send(master.target_system, master.target_component, b'MPC_Z_P', MPC_Z_P, mavutil.mavlink.MAV_PARAM_TYPE_REAL32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda6a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def px4_get_telemetry_data():\n",
    "    ack_msg = master.recv_match(type='COMMAND_ACK')\n",
    "    observation=[]\n",
    "    t_param='SCALED_IMU' #26\n",
    "    try: \n",
    "        SCALED_IMU_xacc = master.messages[t_param].xacc\n",
    "        SCALED_IMU_yacc = master.messages[t_param].yacc\n",
    "        SCALED_IMU_zacc = master.messages[t_param].zacc\n",
    "        SCALED_IMU_xgyro = master.messages[t_param].xgyro\n",
    "        SCALED_IMU_ygyro = master.messages[t_param].ygyro\n",
    "        SCALED_IMU_zgyro = master.messages[t_param].zgyro\n",
    "        SCALED_IMU_data=[SCALED_IMU_xacc/16384,SCALED_IMU_yacc/16384,SCALED_IMU_zacc/16384,SCALED_IMU_xgyro/16384,SCALED_IMU_ygyro/16384,SCALED_IMU_zgyro/16384]\n",
    "        observation.extend(SCALED_IMU_data) #count=6\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='ATTITUDE' #30\n",
    "    try: \n",
    "        ATTITUDE_roll = master.messages[t_param].roll\n",
    "        ATTITUDE_pitch = master.messages[t_param].pitch\n",
    "        ATTITUDE_yaw = master.messages[t_param].yaw\n",
    "        ATTITUDE_rollspeed = master.messages[t_param].rollspeed\n",
    "        ATTITUDE_pitchspeed = master.messages[t_param].pitchspeed\n",
    "        ATTITUDE_yawspeed = master.messages[t_param].yawspeed\n",
    "        ATTITUDE_data=[ATTITUDE_roll/2,ATTITUDE_pitch/2,ATTITUDE_yaw/2,ATTITUDE_rollspeed/5,ATTITUDE_pitchspeed/5,ATTITUDE_yawspeed/5]\n",
    "        observation.extend(ATTITUDE_data) #count=6+6=12\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received') #6\n",
    "\n",
    "    t_param='ATTITUDE_QUATERNION' #31\n",
    "    try: \n",
    "        ATTITUDE_QUATERNION_q1 = master.messages[t_param].q1\n",
    "        ATTITUDE_QUATERNION_q2 = master.messages[t_param].q2\n",
    "        ATTITUDE_QUATERNION_q3 = master.messages[t_param].q3\n",
    "        ATTITUDE_QUATERNION_q4 = master.messages[t_param].q4\n",
    "        ATTITUDE_QUATERNION_rollspeed = master.messages[t_param].rollspeed\n",
    "        ATTITUDE_QUATERNION_pitchspeed = master.messages[t_param].pitchspeed\n",
    "        ATTITUDE_QUATERNION_yawspeed = master.messages[t_param].yawspeed\n",
    "        ATTITUDE_QUATERNION_data=[ATTITUDE_QUATERNION_q1,ATTITUDE_QUATERNION_q2,ATTITUDE_QUATERNION_q3,ATTITUDE_QUATERNION_q4,ATTITUDE_QUATERNION_rollspeed/5,ATTITUDE_QUATERNION_pitchspeed/5,ATTITUDE_QUATERNION_yawspeed/5]\n",
    "        observation.extend(ATTITUDE_QUATERNION_data) #count=12+7=19\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='LOCAL_POSITION_NED' #32\n",
    "    try: \n",
    "        LOCAL_POSITION_NED_x = master.messages[t_param].x\n",
    "        LOCAL_POSITION_NED_y = master.messages[t_param].y\n",
    "        LOCAL_POSITION_NED_z = master.messages[t_param].z + 10\n",
    "        LOCAL_POSITION_NED_vx = master.messages[t_param].vx\n",
    "        LOCAL_POSITION_NED_vy = master.messages[t_param].vy\n",
    "        LOCAL_POSITION_NED_vz = master.messages[t_param].vz\n",
    "        LOCAL_POSITION_NED_data=[LOCAL_POSITION_NED_x/10,LOCAL_POSITION_NED_y/10,(LOCAL_POSITION_NED_z+target_elevation)/10,LOCAL_POSITION_NED_vx/10,LOCAL_POSITION_NED_vy/10,LOCAL_POSITION_NED_vz/10]\n",
    "        observation.extend(LOCAL_POSITION_NED_data) #count=19+6=25\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='SERVO_OUTPUT_RAW' #36\n",
    "    try: \n",
    "        SERVO_OUTPUT_RAW_servo1_raw = master.messages[t_param].servo1_raw - 1500\n",
    "        SERVO_OUTPUT_RAW_servo2_raw = master.messages[t_param].servo2_raw - 1500\n",
    "        SERVO_OUTPUT_RAW_servo3_raw = master.messages[t_param].servo3_raw - 1500\n",
    "        SERVO_OUTPUT_RAW_servo4_raw = master.messages[t_param].servo4_raw - 1500\n",
    "        SERVO_OUTPUT_RAW_data=[SERVO_OUTPUT_RAW_servo1_raw/500,SERVO_OUTPUT_RAW_servo2_raw/500,SERVO_OUTPUT_RAW_servo3_raw/500,SERVO_OUTPUT_RAW_servo4_raw/500]\n",
    "        observation.extend(SERVO_OUTPUT_RAW_data) #count=25+4=29\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='VFR_HUD' #74\n",
    "    try: \n",
    "        #VFR_HUD_airspeed = master.messages[t_param].airspeed\n",
    "        VFR_HUD_groundspeed = master.messages[t_param].groundspeed\n",
    "        VFR_HUD_heading = master.messages[t_param].heading\n",
    "        VFR_HUD_throttle = master.messages[t_param].throttle\n",
    "        #VFR_HUD_alt = master.messages[t_param].alt\n",
    "        VFR_HUD_climb = master.messages[t_param].climb\n",
    "        VFR_HUD_data=[VFR_HUD_groundspeed/10,VFR_HUD_heading/360,VFR_HUD_throttle/100,VFR_HUD_climb/10]\n",
    "        observation.extend(VFR_HUD_data) #count=29+4=33\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "        \n",
    "    t_param='ATTITUDE_TARGET' #83\n",
    "    try: \n",
    "        ATTITUDE_TARGET_q = master.messages[t_param].q\n",
    "        ATTITUDE_TARGET_body_roll_rate = master.messages[t_param].body_roll_rate\n",
    "        ATTITUDE_TARGET_body_pitch_rate = master.messages[t_param].body_pitch_rate\n",
    "        ATTITUDE_TARGET_body_yaw_rate = master.messages[t_param].body_yaw_rate\n",
    "        ATTITUDE_TARGET_thrust = master.messages[t_param].thrust\n",
    "        ATTITUDE_TARGET_data=[ATTITUDE_TARGET_q[0],ATTITUDE_TARGET_q[1],ATTITUDE_TARGET_q[2],ATTITUDE_TARGET_q[3],ATTITUDE_TARGET_body_roll_rate/4,ATTITUDE_TARGET_body_pitch_rate/4,ATTITUDE_TARGET_body_yaw_rate/4,ATTITUDE_TARGET_thrust]\n",
    "        observation.extend(ATTITUDE_TARGET_data) #count=33+8=41\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='ESTIMATOR_STATUS' #230\n",
    "    try: \n",
    "        ESTIMATOR_STATUS_vel_ratio = master.messages[t_param].vel_ratio\n",
    "        ESTIMATOR_STATUS_pos_horiz_ratio = master.messages[t_param].pos_horiz_ratio\n",
    "        ESTIMATOR_STATUS_pos_vert_ratio = master.messages[t_param].pos_vert_ratio\n",
    "        ESTIMATOR_STATUS_mag_ratio = master.messages[t_param].mag_ratio\n",
    "        ESTIMATOR_STATUS_data=[ESTIMATOR_STATUS_vel_ratio/5,ESTIMATOR_STATUS_pos_horiz_ratio,ESTIMATOR_STATUS_pos_vert_ratio,ESTIMATOR_STATUS_mag_ratio]\n",
    "        observation.extend(ESTIMATOR_STATUS_data) #count=41+4=45\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='UTM_GLOBAL_POSITION' #340\n",
    "    try: \n",
    "        UTM_GLOBAL_POSITION_vx = master.messages[t_param].vx\n",
    "        UTM_GLOBAL_POSITION_vy = master.messages[t_param].vy\n",
    "        UTM_GLOBAL_POSITION_vz = master.messages[t_param].vz\n",
    "        UTM_GLOBAL_POSITION_data=[UTM_GLOBAL_POSITION_vx/1000, UTM_GLOBAL_POSITION_vy/1000, UTM_GLOBAL_POSITION_vz/1000]\n",
    "        observation.extend(UTM_GLOBAL_POSITION_data) #count=45+3=48\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    t_param='OPEN_DRONE_ID_LOCATION' #12901\n",
    "    try: \n",
    "        OPEN_DRONE_ID_LOCATION_speed_horizontal = master.messages[t_param].speed_horizontal\n",
    "        OPEN_DRONE_ID_LOCATION_speed_vertical = master.messages[t_param].speed_vertical\n",
    "        OPEN_DRONE_ID_LOCATION_data=[OPEN_DRONE_ID_LOCATION_speed_horizontal/1000, OPEN_DRONE_ID_LOCATION_speed_vertical/1000]\n",
    "        observation.extend(OPEN_DRONE_ID_LOCATION_data) #count=48+2=50\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "\n",
    "    return observation\n",
    "\n",
    "def px4_get_position_data():\n",
    "    ack_msg = master.recv_match(type='COMMAND_ACK')\n",
    "    t_param='LOCAL_POSITION_NED' #32\n",
    "    try: \n",
    "        LOCAL_POSITION_NED_x = master.messages[t_param].x\n",
    "        LOCAL_POSITION_NED_y = master.messages[t_param].y\n",
    "        LOCAL_POSITION_NED_z = master.messages[t_param].z\n",
    "        return [LOCAL_POSITION_NED_x, LOCAL_POSITION_NED_y, LOCAL_POSITION_NED_z]\n",
    "    except:\n",
    "        print(t_param, \":\", 'No message received')\n",
    "        return [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73afd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PX4_RL_PID(py_environment.PyEnvironment):\n",
    "    def __init__(self, noSample, delay_period, action_period):\n",
    "        self.sample_size = noSample\n",
    "        self.delay_period = delay_period\n",
    "        self.action_period = action_period\n",
    "        self.noObs = 50\n",
    "        self.sample_stamp = 0\n",
    "        self.track_distance = 0\n",
    "        self.max_distance = 0\n",
    "        self.action_history = []\n",
    "        self.observation_history = []\n",
    "        self.reward_accumulate = 0\n",
    "        tActionMin=[0.3, 0.0004, 0.1, 0.3, 0.0004, 0.1, 0.3, 0.04,  1.0,  1.0, 1.0, 1.2,  0.2, 0.1,  2.0, 0.2, 0.0, 0.0, 0.0]\n",
    "        tActionMax=[3.0,   0.01, 0.5, 3.0,   0.01, 0.5, 3.0,  0.4, 14.0, 14.0, 5.0, 5.0, 10.0, 2.0, 15.0, 3.0, 2.0, 2.0, 2.0]\n",
    "        tObsMin=[-1.0] * self.noObs\n",
    "        tObsMax=[ 1.0] * self.noObs\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(19,), dtype=np.float32, minimum=tActionMin, maximum=tActionMax, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(self.noObs,), dtype=np.float32, minimum=tObsMin, maximum=tObsMax, name='observation')\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state = 0\n",
    "        self._episode_ended = False\n",
    "        self.sample_stamp = 0\n",
    "        self.track_distance = 0\n",
    "        self.max_distance = 0\n",
    "        self.action_history = []\n",
    "        self.observation_history = []\n",
    "        self.reward_accumulate = 0\n",
    "        return ts.restart(np.array([0.0] * self.noObs, dtype=np.float32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        #sample_period=0.1\n",
    "        self.action_history.append(action)\n",
    "\n",
    "        px4_update_pid_controller_all(action)\n",
    "        #print(\"Action:\", action)\n",
    "        if self.sample_stamp == 0:\n",
    "            self.sample_stamp = time.time()\n",
    "\n",
    "        #wait for sample period\n",
    "        while time.time() - self.sample_stamp < self.action_period:\n",
    "            time.sleep(self.delay_period)\n",
    "        self.sample_stamp = time.time()\n",
    "    \n",
    "        px4_set_target_depth_local(-target_elevation)\n",
    "        observation = px4_get_telemetry_data()\n",
    "        self.observation_history.append(observation)\n",
    "\n",
    "        position = px4_get_position_data()\n",
    "        #print(\"Position:\", self._state, position)\n",
    "        distance = distance_target([position[0], position[1], position[2]])\n",
    "        self.track_distance  += distance\n",
    "        if self.max_distance < distance:\n",
    "            self.max_distance = distance\n",
    "\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        self._state += 1\n",
    "        if self._state > self.sample_size:\n",
    "            self._episode_ended = True\n",
    "\n",
    "        current_reward = 1/(1+distance)/(1+self.track_distance/self._state)/(1+self.max_distance)\n",
    "        self.reward_accumulate += current_reward\n",
    "\n",
    "        if self._episode_ended == True:\n",
    "            #reward = current_reward\n",
    "            print(\"Distance: %.3f / Max distance: %.3f / Reward: %.3f, \" % (self.track_distance/self.sample_size, self.max_distance, self.reward_accumulate))\n",
    "            f=open(\"px4_rl_output\"+str(self.sample_size)+\".txt\", 'a')\n",
    "            f.write(\"%.3f\\t%.3f\\t%.3f\\n\" % (self.track_distance/self.sample_size, self.max_distance, self.reward_accumulate))\n",
    "            f.close()\n",
    "            f=open(\"px4_rl_action\"+str(self.sample_size)+\".txt\", 'w')\n",
    "            for items in self.action_history:\n",
    "                for item in items:\n",
    "                    f.write(\"%.3f\\t\" % item)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()            \n",
    "            f=open(\"px4_rl_observation\"+str(self.sample_size)+\".txt\", 'w')\n",
    "            for items in self.observation_history:\n",
    "                for item in items:\n",
    "                    f.write(\"%.3f\\t\" % item)\n",
    "                f.write(\"\\n\")\n",
    "            f.close()            \n",
    "            return ts.termination(np.array(observation, dtype=np.float32), current_reward)\n",
    "        else:\n",
    "            return ts.transition(\n",
    "                np.array(observation, dtype=np.float32), reward=current_reward, discount=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47701546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "    driver = py_driver.PyDriver(\n",
    "        environment,\n",
    "        py_tf_eager_policy.PyTFEagerPolicy(\n",
    "        policy, use_tf_function=True),\n",
    "        [rb_observer],\n",
    "        max_episodes=num_episodes)\n",
    "    initial_time_step = environment.reset()\n",
    "    driver.run(initial_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6f71521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Setup PX4----------\n",
      "Heartbeat: (system 1 component 0)\n",
      "State: arm\n",
      "State: set_mode\n",
      "State: mode_wait\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n"
     ]
    }
   ],
   "source": [
    "print(\"-----Setup PX4----------\")\n",
    "########################################\n",
    "master = mavutil.mavlink_connection('udpin:localhost:14540')\n",
    "boot_time = time.time()\n",
    "master.wait_heartbeat()\n",
    "print(\"Heartbeat: (system %u component %u)\" % (master.target_system, master.target_component))\n",
    "########################################\n",
    "px4_reset_pid_controller()\n",
    "px4_arm()\n",
    "px4_request_message()\n",
    "px4_steady_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0e99f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Instantiate PID----------\n",
      "-----TF setup----------\n",
      "-----TF agent----------\n",
      "-----Reverve----------\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "print(\"-----Instantiate PID----------\")\n",
    "learning_rate = 1e-5 # @param {type:\"number\"}\n",
    "collect_episodes_per_iteration = 5 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 5000 # @param {type:\"integer\"}\n",
    "fc_layer_params = (200,400,200,100,50)\n",
    "\n",
    "train_duration=60 #sec\n",
    "evaluation_duration=120 #sec\n",
    "train_no_sample=int(train_duration/period_action)\n",
    "eval_no_sample=int(evaluation_duration/period_action)\n",
    "train_py_env = PX4_RL_PID(train_no_sample, period_delay, period_action)\n",
    "eval_py_env = PX4_RL_PID(eval_no_sample, period_delay, period_action)\n",
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "log_interval = 5 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 31# @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}\n",
    "checkpoint_interval = 10\n",
    "returns = []\n",
    "\n",
    "print(\"-----TF setup----------\")\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "print(\"-----TF agent----------\")\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    #train_step_counter=global_step)\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "print(\"-----Reverve----------\")\n",
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "    tf_agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=None,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddEpisodeObserver(\n",
    "    replay_buffer.py_client,\n",
    "    table_name,\n",
    "    replay_buffer_capacity\n",
    ")\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Checkpoint saver\n",
    "tempdir='.'\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step_counter\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72a9459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Evaluate----------\n",
      "Distance: 0.259 / Max distance: 1.035 / Reward: 21400.908, \n",
      "Distance: 0.271 / Max distance: 2.328 / Reward: 49819.050, \n",
      "Distance: 0.256 / Max distance: 0.883 / Reward: 76012.138, \n",
      "Distance: 0.248 / Max distance: 1.316 / Reward: 127732.326, \n",
      "Distance: 0.336 / Max distance: 1.844 / Reward: 106500.057, \n",
      "Distance: 0.256 / Max distance: 0.977 / Reward: 55431.721, \n",
      "Distance: 0.269 / Max distance: 1.067 / Reward: 83076.929, \n",
      "Distance: 0.281 / Max distance: 1.810 / Reward: 99261.032, \n",
      "Distance: 0.243 / Max distance: 0.790 / Reward: 41777.305, \n",
      "Distance: 0.289 / Max distance: 2.082 / Reward: 37081.270, \n",
      "Distance: 0.253 / Max distance: 0.888 / Reward: 62316.447, \n",
      "Distance: 0.230 / Max distance: 0.869 / Reward: 86839.152, \n",
      "Distance: 0.257 / Max distance: 1.344 / Reward: 65146.251, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Evaluate the agent's policy once before training.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm px4_rl_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(eval_no_sample)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m avg_return \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_avg_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_eval_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmv px4_rl_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(eval_no_sample)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt px4_rl_output\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(eval_no_sample)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_0.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m returns \u001b[38;5;241m=\u001b[39m [avg_return]\n",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m, in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m time_step\u001b[38;5;241m.\u001b[39mis_last():\n\u001b[1;32m      9\u001b[0m     action_step \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39maction(time_step)\n\u001b[0;32m---> 10\u001b[0m     time_step \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     episode_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time_step\u001b[38;5;241m.\u001b[39mreward\n\u001b[1;32m     12\u001b[0m total_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m episode_return\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/tf_environment.py:241\u001b[0m, in \u001b[0;36mTFEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Steps the environment according to the action.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m  If the environment returned a `TimeStep` with `StepType.LAST` at the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m        corresponding to `observation_spec()`.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/tf_py_environment.py:315\u001b[0m, in \u001b[0;36mTFPyEnvironment._step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (action\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    310\u001b[0m         (dim_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim_value \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)):\n\u001b[1;32m    311\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected actions whose major dimension is batch_size (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    313\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbut saw action with shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    314\u001b[0m           (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, action\u001b[38;5;241m.\u001b[39mshape, action))\n\u001b[0;32m--> 315\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_isolated_step_py\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_time_step_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep_py_func\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step_from_numpy_function_outputs(outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py:767\u001b[0m, in \u001b[0;36mnumpy_function\u001b[0;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy_function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy_function\u001b[39m(func, inp, Tout, stateful\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Wraps a python function and uses it as a TensorFlow op.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m  Given a python function `func` wrap this function as an operation in a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m    Single or list of `tf.Tensor` which `func` computes.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpy_func_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstateful\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstateful\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py:633\u001b[0m, in \u001b[0;36mpy_func_common\u001b[0;34m(func, inp, Tout, stateful, name)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wraps a python function and uses it as a TensorFlow op.\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03mGiven a python function `func`, which takes numpy arrays as its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 633\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m   result \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(result)\n\u001b[1;32m    636\u001b[0m   result \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/tf_py_environment.py:302\u001b[0m, in \u001b[0;36mTFPyEnvironment._step.<locals>._isolated_step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_isolated_step_py\u001b[39m(\u001b[38;5;241m*\u001b[39mflattened_actions):\n\u001b[0;32m--> 302\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_step_py\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflattened_actions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/tf_py_environment.py:211\u001b[0m, in \u001b[0;36mTFPyEnvironment._execute\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    210\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mapply(fn, args\u001b[38;5;241m=\u001b[39margs, kwds\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/tf_py_environment.py:298\u001b[0m, in \u001b[0;36mTFPyEnvironment._step.<locals>._step_py\u001b[0;34m(*flattened_actions)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _check_not_called_concurrently(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock):\n\u001b[1;32m    296\u001b[0m   packed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m    297\u001b[0m       structure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_spec(), flat_sequence\u001b[38;5;241m=\u001b[39mflattened_actions)\n\u001b[0;32m--> 298\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_step)\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/py_environment.py:232\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step)):\n\u001b[1;32m    230\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/batched_py_environment.py:165\u001b[0m, in \u001b[0;36mBatchedPyEnvironment._step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_envs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    164\u001b[0m   actions \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39munbatch_nested_array(actions)\n\u001b[0;32m--> 165\u001b[0m   time_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_envs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest_utils\u001b[38;5;241m.\u001b[39mbatch_nested_array(time_steps)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/agents-master/tf_agents/environments/py_environment.py:232\u001b[0m, in \u001b[0;36mPyEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step)):\n\u001b[1;32m    230\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_time_step\n",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m, in \u001b[0;36mPX4_RL_PID._step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#wait for sample period\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_stamp \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_period:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelay_period\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_stamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     55\u001b[0m px4_set_target_depth_local(\u001b[38;5;241m-\u001b[39mtarget_elevation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"-----Evaluate----------\")\n",
    "# Evaluate the agent's policy once before training.\n",
    "os.system(\"rm px4_rl_output\"+str(eval_no_sample)+\".txt\")\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "os.system(\"mv px4_rl_output\"+str(eval_no_sample)+\".txt px4_rl_output\"+str(eval_no_sample)+\"_0.txt\")\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "344fec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'global_step:0' shape=() dtype=int64, numpy=0>\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint saver\n",
    "tempdir='.'\n",
    "checkpoint_dir = os.path.join(tempdir, 'checkpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=tf_agent,\n",
    "    policy=tf_agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step_counter\n",
    "    )\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "train_step_counter = tf.compat.v1.train.get_global_step()\n",
    "print(train_step_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "143a7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'global_step:0' shape=() dtype=int64, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "print(train_step_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806858a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Train----------\n",
      "Iteration: 0 / 0\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 0.405 / Max distance: 1.773 / Reward: 149.287, \n",
      "Distance: 0.832 / Max distance: 5.220 / Reward: 105.638, \n",
      "Distance: 0.866 / Max distance: 7.940 / Reward: 112.739, \n",
      "Distance: 6.014 / Max distance: 25.965 / Reward: 53.060, \n",
      "Distance: 0.594 / Max distance: 3.503 / Reward: 114.178, \n",
      "Loss: 23.579\n",
      "Iteration: 1 / 1\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 1.056 / Max distance: 5.911 / Reward: 42.525, \n",
      "Distance: 2.076 / Max distance: 10.826 / Reward: 7.369, \n",
      "Distance: 1.599 / Max distance: 13.504 / Reward: 7.094, \n",
      "Distance: 0.805 / Max distance: 5.594 / Reward: 67.304, \n",
      "Distance: 0.540 / Max distance: 4.266 / Reward: 148.531, \n",
      "Loss: 4.629\n",
      "Iteration: 2 / 2\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 3.565 / Max distance: 17.429 / Reward: 78.899, \n",
      "Distance: 0.813 / Max distance: 6.077 / Reward: 27.900, \n",
      "Distance: 0.468 / Max distance: 1.760 / Reward: 129.553, \n",
      "Distance: 3.308 / Max distance: 17.304 / Reward: 25.054, \n",
      "Distance: 2.825 / Max distance: 15.403 / Reward: 97.194, \n",
      "Loss: 79.790\n",
      "Iteration: 3 / 3\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.537 / Max distance: 3.159 / Reward: 118.261, \n",
      "Distance: 0.470 / Max distance: 2.269 / Reward: 109.138, \n",
      "Distance: 1.165 / Max distance: 7.172 / Reward: 86.463, \n",
      "Distance: 0.753 / Max distance: 4.099 / Reward: 38.829, \n",
      "Distance: 0.293 / Max distance: 1.695 / Reward: 178.227, \n",
      "Loss: 81.881\n",
      "Iteration: 4 / 4\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 1.147 / Max distance: 4.041 / Reward: 52.473, \n",
      "Distance: 0.494 / Max distance: 2.639 / Reward: 119.099, \n",
      "Distance: 0.732 / Max distance: 3.418 / Reward: 78.672, \n",
      "Distance: 0.366 / Max distance: 1.220 / Reward: 189.406, \n",
      "Distance: 0.467 / Max distance: 4.024 / Reward: 150.181, \n",
      "Loss: 17.043\n",
      "Iteration: 5 / 5\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.963 / Max distance: 5.763 / Reward: 127.511, \n",
      "Distance: 0.401 / Max distance: 1.443 / Reward: 143.444, \n",
      "Distance: 0.473 / Max distance: 2.428 / Reward: 146.204, \n",
      "Distance: 0.632 / Max distance: 4.046 / Reward: 85.876, \n",
      "Distance: 1.181 / Max distance: 8.779 / Reward: 136.878, \n",
      "Loss: 51.820\n",
      "Iteration: 6 / 6\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 2.574 / Max distance: 16.334 / Reward: 26.135, \n",
      "Distance: 0.575 / Max distance: 3.459 / Reward: 110.062, \n",
      "Distance: 0.589 / Max distance: 3.799 / Reward: 71.162, \n",
      "Distance: 0.702 / Max distance: 3.941 / Reward: 41.051, \n",
      "Distance: 1.181 / Max distance: 9.146 / Reward: 96.925, \n",
      "Loss: 57.623\n",
      "Iteration: 7 / 7\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 2.823 / Max distance: 18.253 / Reward: 61.239, \n",
      "Distance: 1.113 / Max distance: 6.093 / Reward: 61.013, \n",
      "Distance: 0.606 / Max distance: 4.568 / Reward: 107.182, \n",
      "Distance: 1.014 / Max distance: 5.661 / Reward: 56.788, \n",
      "Distance: 0.413 / Max distance: 2.629 / Reward: 175.892, \n",
      "Loss: -5.232\n",
      "Iteration: 8 / 8\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.671 / Max distance: 3.642 / Reward: 94.628, \n",
      "Distance: 0.314 / Max distance: 1.020 / Reward: 188.854, \n",
      "Distance: 0.779 / Max distance: 4.054 / Reward: 98.375, \n",
      "Distance: 0.518 / Max distance: 1.785 / Reward: 110.496, \n",
      "Distance: 0.437 / Max distance: 1.844 / Reward: 186.224, \n",
      "Loss: 42.910\n",
      "Iteration: 9 / 9\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 1.976 / Max distance: 15.491 / Reward: 110.877, \n",
      "Distance: 0.937 / Max distance: 10.788 / Reward: 122.689, \n",
      "Distance: 1.805 / Max distance: 11.653 / Reward: 6.933, \n",
      "Distance: 0.660 / Max distance: 5.078 / Reward: 61.907, \n",
      "Distance: 0.419 / Max distance: 1.892 / Reward: 172.147, \n",
      "Loss: -15.505\n",
      "Iteration: 10 / 10\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 1.181 / Max distance: 9.783 / Reward: 78.300, \n",
      "Distance: 1.074 / Max distance: 5.888 / Reward: 26.918, \n",
      "Distance: 0.551 / Max distance: 2.426 / Reward: 86.234, \n",
      "Distance: 0.760 / Max distance: 3.082 / Reward: 92.071, \n",
      "Distance: 0.692 / Max distance: 4.315 / Reward: 49.028, \n",
      "Loss: 153.599\n",
      "Iteration: 11 / 11\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 0.732 / Max distance: 3.764 / Reward: 78.054, \n",
      "Distance: 0.482 / Max distance: 4.268 / Reward: 104.615, \n",
      "Distance: 6.555 / Max distance: 26.738 / Reward: 80.488, \n",
      "Distance: 0.630 / Max distance: 3.728 / Reward: 71.855, \n",
      "Distance: 0.850 / Max distance: 4.336 / Reward: 76.259, \n",
      "Loss: -87.159\n",
      "Iteration: 12 / 12\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.534 / Max distance: 2.033 / Reward: 107.716, \n",
      "Distance: 0.819 / Max distance: 4.066 / Reward: 80.736, \n",
      "Distance: 0.478 / Max distance: 1.812 / Reward: 104.634, \n",
      "Distance: 0.395 / Max distance: 1.710 / Reward: 170.446, \n",
      "Distance: 0.980 / Max distance: 7.393 / Reward: 122.350, \n",
      "Loss: -57.671\n",
      "Iteration: 13 / 13\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.602 / Max distance: 3.069 / Reward: 77.362, \n",
      "Distance: 0.976 / Max distance: 5.615 / Reward: 46.371, \n",
      "Distance: 0.718 / Max distance: 3.126 / Reward: 59.731, \n",
      "Distance: 0.432 / Max distance: 1.240 / Reward: 148.771, \n",
      "Distance: 3.853 / Max distance: 27.171 / Reward: 81.491, \n",
      "Loss: -16.466\n",
      "Iteration: 14 / 14\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.459 / Max distance: 1.598 / Reward: 129.807, \n",
      "Distance: 0.525 / Max distance: 3.121 / Reward: 89.753, \n",
      "Distance: 2.800 / Max distance: 14.373 / Reward: 6.932, \n",
      "Distance: 2.075 / Max distance: 13.729 / Reward: 72.438, \n",
      "Distance: 1.921 / Max distance: 13.631 / Reward: 33.569, \n",
      "Loss: 33.801\n",
      "Iteration: 15 / 15\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.511 / Max distance: 3.457 / Reward: 81.940, \n",
      "Distance: 2.733 / Max distance: 13.989 / Reward: 79.466, \n",
      "Distance: 0.669 / Max distance: 4.016 / Reward: 78.319, \n",
      "Distance: 0.519 / Max distance: 3.076 / Reward: 102.749, \n",
      "Distance: 0.869 / Max distance: 7.034 / Reward: 99.178, \n",
      "Loss: -28.136\n",
      "Iteration: 16 / 16\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 2.117 / Max distance: 13.158 / Reward: 22.278, \n",
      "Distance: 0.509 / Max distance: 2.247 / Reward: 136.654, \n",
      "Distance: 1.031 / Max distance: 9.366 / Reward: 89.150, \n",
      "Distance: 0.368 / Max distance: 1.175 / Reward: 165.909, \n",
      "Distance: 0.496 / Max distance: 2.917 / Reward: 133.665, \n",
      "Loss: 141.464\n",
      "Iteration: 17 / 17\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.802 / Max distance: 4.141 / Reward: 66.293, \n",
      "Distance: 1.387 / Max distance: 7.827 / Reward: 30.989, \n",
      "Distance: 0.436 / Max distance: 2.685 / Reward: 184.056, \n",
      "Distance: 0.915 / Max distance: 5.027 / Reward: 58.315, \n",
      "Distance: 1.879 / Max distance: 10.029 / Reward: 8.170, \n",
      "Loss: -37.030\n",
      "Iteration: 18 / 18\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.331 / Max distance: 0.837 / Reward: 194.307, \n",
      "Distance: 0.383 / Max distance: 1.493 / Reward: 179.777, \n",
      "Distance: 0.446 / Max distance: 2.263 / Reward: 115.079, \n",
      "Distance: 1.682 / Max distance: 13.441 / Reward: 77.650, \n",
      "Distance: 0.691 / Max distance: 4.066 / Reward: 46.554, \n",
      "Loss: 71.286\n",
      "Iteration: 19 / 19\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.550 / Max distance: 1.805 / Reward: 108.357, \n",
      "Distance: 0.374 / Max distance: 1.291 / Reward: 135.845, \n",
      "Distance: 0.503 / Max distance: 3.220 / Reward: 132.990, \n",
      "Distance: 0.757 / Max distance: 5.319 / Reward: 97.328, \n",
      "Distance: 0.275 / Max distance: 0.864 / Reward: 232.825, \n",
      "Loss: -21.967\n",
      "Iteration: 20 / 20\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 0.977 / Max distance: 7.655 / Reward: 49.650, \n",
      "Distance: 0.715 / Max distance: 7.650 / Reward: 22.573, \n",
      "Distance: 0.883 / Max distance: 8.870 / Reward: 128.808, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 1.476 / Max distance: 8.251 / Reward: 11.729, \n",
      "Distance: 1.086 / Max distance: 9.121 / Reward: 83.910, \n",
      "Loss: -62.130\n",
      "Iteration: 21 / 21\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.666 / Max distance: 5.349 / Reward: 145.640, \n",
      "Distance: 5.807 / Max distance: 59.895 / Reward: 159.187, \n",
      "Distance: 18.367 / Max distance: 71.997 / Reward: 0.097, \n",
      "Distance: 0.536 / Max distance: 2.694 / Reward: 111.274, \n",
      "Distance: 0.450 / Max distance: 2.075 / Reward: 119.703, \n",
      "Loss: -103.974\n",
      "Iteration: 22 / 22\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 0.667 / Max distance: 2.620 / Reward: 79.930, \n",
      "Distance: 0.743 / Max distance: 3.721 / Reward: 84.634, \n",
      "Distance: 2.980 / Max distance: 23.093 / Reward: 94.138, \n",
      "Distance: 0.788 / Max distance: 5.027 / Reward: 81.223, \n",
      "Distance: 1.971 / Max distance: 12.387 / Reward: 89.082, \n",
      "Loss: -42.694\n",
      "Iteration: 23 / 23\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.833 / Max distance: 3.969 / Reward: 77.873, \n",
      "Distance: 0.734 / Max distance: 6.896 / Reward: 43.163, \n",
      "Distance: 4.385 / Max distance: 26.338 / Reward: 32.766, \n",
      "Distance: 0.417 / Max distance: 1.481 / Reward: 116.217, \n",
      "Distance: 0.456 / Max distance: 1.782 / Reward: 114.419, \n",
      "Loss: 70.183\n",
      "Iteration: 24 / 24\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 1.097 / Max distance: 7.939 / Reward: 94.686, \n",
      "Distance: 0.411 / Max distance: 2.409 / Reward: 129.169, \n",
      "Distance: 1.447 / Max distance: 8.681 / Reward: 54.287, \n",
      "Distance: 1.416 / Max distance: 8.710 / Reward: 116.286, \n",
      "Distance: 1.123 / Max distance: 7.156 / Reward: 47.454, \n",
      "Loss: 65.840\n",
      "Iteration: 25 / 25\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.470 / Max distance: 2.838 / Reward: 69.580, \n",
      "Distance: 0.517 / Max distance: 4.904 / Reward: 158.453, \n",
      "Distance: 1.131 / Max distance: 6.516 / Reward: 19.679, \n",
      "Distance: 0.928 / Max distance: 5.867 / Reward: 47.865, \n",
      "Distance: 0.624 / Max distance: 2.931 / Reward: 68.220, \n",
      "Loss: 31.827\n",
      "Iteration: 26 / 26\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.400 / Max distance: 2.267 / Reward: 101.363, \n",
      "Distance: 0.353 / Max distance: 1.432 / Reward: 154.983, \n",
      "Distance: 1.852 / Max distance: 14.533 / Reward: 91.006, \n",
      "Distance: 0.470 / Max distance: 2.009 / Reward: 121.894, \n",
      "Distance: 0.542 / Max distance: 3.012 / Reward: 105.956, \n",
      "Loss: -62.404\n",
      "Iteration: 27 / 27\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.450 / Max distance: 1.844 / Reward: 128.687, \n",
      "Distance: 0.920 / Max distance: 7.316 / Reward: 80.105, \n",
      "Distance: 0.606 / Max distance: 2.444 / Reward: 69.444, \n",
      "Distance: 0.606 / Max distance: 6.600 / Reward: 141.357, \n",
      "Distance: 0.591 / Max distance: 3.614 / Reward: 96.007, \n",
      "Loss: -21.064\n",
      "Iteration: 28 / 28\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.890 / Max distance: 6.481 / Reward: 90.594, \n",
      "Distance: 0.946 / Max distance: 5.940 / Reward: 71.612, \n",
      "Distance: 0.295 / Max distance: 1.120 / Reward: 212.210, \n",
      "Distance: 0.390 / Max distance: 1.400 / Reward: 209.157, \n",
      "Distance: 0.811 / Max distance: 5.137 / Reward: 93.289, \n",
      "Loss: -66.650\n",
      "Iteration: 29 / 29\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.458 / Max distance: 2.070 / Reward: 115.952, \n",
      "Distance: 0.523 / Max distance: 2.082 / Reward: 104.147, \n",
      "Distance: 0.472 / Max distance: 4.768 / Reward: 172.730, \n",
      "Distance: 1.054 / Max distance: 8.256 / Reward: 74.374, \n",
      "Distance: 1.833 / Max distance: 9.388 / Reward: 39.765, \n",
      "Loss: -31.683\n",
      "Iteration: 30 / 30\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 1.723 / Max distance: 11.900 / Reward: 45.786, \n",
      "Distance: 0.428 / Max distance: 2.636 / Reward: 104.564, \n",
      "Distance: 1.337 / Max distance: 10.537 / Reward: 60.568, \n",
      "Distance: 0.936 / Max distance: 7.400 / Reward: 118.092, \n",
      "Distance: 0.541 / Max distance: 2.372 / Reward: 113.954, \n",
      "Loss: -112.725\n",
      "Iteration: 31 / 31\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.371 / Max distance: 1.825 / Reward: 127.788, \n",
      "Distance: 0.532 / Max distance: 2.059 / Reward: 94.709, \n",
      "Distance: 0.748 / Max distance: 5.728 / Reward: 112.401, \n",
      "Distance: 0.372 / Max distance: 2.390 / Reward: 203.086, \n",
      "Distance: 0.798 / Max distance: 7.117 / Reward: 73.539, \n",
      "Loss: 29.203\n",
      "Iteration: 32 / 32\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 1.955 / Max distance: 16.812 / Reward: 52.257, \n",
      "Distance: 1.593 / Max distance: 7.487 / Reward: 57.319, \n",
      "Distance: 0.302 / Max distance: 1.433 / Reward: 238.150, \n",
      "Distance: 0.453 / Max distance: 2.185 / Reward: 124.251, \n",
      "Distance: 2.951 / Max distance: 16.737 / Reward: 19.586, \n",
      "Loss: 12.986\n",
      "Iteration: 33 / 33\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 1.072 / Max distance: 8.681 / Reward: 86.970, \n",
      "Distance: 0.746 / Max distance: 5.543 / Reward: 124.552, \n",
      "Distance: 1.658 / Max distance: 13.197 / Reward: 46.230, \n",
      "Distance: 0.621 / Max distance: 3.043 / Reward: 126.576, \n",
      "Distance: 0.427 / Max distance: 2.034 / Reward: 115.546, \n",
      "Loss: 122.487\n",
      "Iteration: 34 / 34\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 1.647 / Max distance: 12.503 / Reward: 133.422, \n",
      "Distance: 0.312 / Max distance: 1.364 / Reward: 143.878, \n",
      "Distance: 0.412 / Max distance: 2.479 / Reward: 160.627, \n",
      "Distance: 0.530 / Max distance: 1.964 / Reward: 92.022, \n",
      "Distance: 0.587 / Max distance: 2.636 / Reward: 84.335, \n",
      "Loss: -59.379\n",
      "Iteration: 35 / 35\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "State: mode_wait\n",
      "Mode: steady_state\n",
      "Distance: 0.566 / Max distance: 3.601 / Reward: 88.166, \n",
      "Distance: 0.489 / Max distance: 2.505 / Reward: 79.002, \n",
      "Distance: 0.664 / Max distance: 3.023 / Reward: 115.121, \n",
      "Distance: 1.124 / Max distance: 7.087 / Reward: 43.915, \n",
      "Distance: 0.354 / Max distance: 1.823 / Reward: 147.154, \n",
      "Loss: 66.647\n",
      "Iteration: 36 / 36\n",
      "State: arm\n",
      "State: set_mode\n",
      "Mode: flight_mode\n",
      "Mode: steady_state\n",
      "Distance: 0.516 / Max distance: 4.566 / Reward: 191.857, \n"
     ]
    }
   ],
   "source": [
    "global_counter = int(tf.compat.v1.train.get_global_step())\n",
    "print(\"-----Train----------\")\n",
    "############################################################\n",
    "for _ in range(num_iterations):\n",
    "    print(\"Iteration:\", _, \"/\", global_counter)\n",
    "    px4_arm()\n",
    "    px4_reset_pid_controller()\n",
    "    px4_steady_state()\n",
    "\n",
    "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "    collect_episode(\n",
    "        train_py_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "    # Use data from the buffer and update the agent's network.\n",
    "    iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))\n",
    "    trajectories, _ = next(iterator)\n",
    "    train_loss = tf_agent.train(experience=trajectories)  \n",
    "\n",
    "    replay_buffer.clear()\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    global_counter = int(tf.compat.v1.train.get_global_step())\n",
    "\n",
    "    train_checkpointer.save(step)\n",
    "    \n",
    "    f=open(\"px4_rl_output_loss.txt\", 'a')\n",
    "    f.write(\"%.3f\\n\" % float(train_loss.loss))\n",
    "    f.close()\n",
    "\n",
    "    #if step % log_interval == 0:\n",
    "    print(\"Loss: %.3f\" % float(train_loss.loss))\n",
    "    \n",
    "    if global_counter % checkpoint_interval == 0:\n",
    "        os.system(\"cp -r checkpoint checkpoint_%d\" % global_counter)  \n",
    "\n",
    "    if global_counter % eval_interval == 0:\n",
    "        os.system(\"rm px4_rl_output\"+str(eval_no_sample)+\".txt\")\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "        os.system(\"mv px4_rl_output\"+str(eval_no_sample)+\".txt px4_rl_output\"+str(eval_no_sample)+\"_%d.txt\" % global_counter)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97c2ad83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Evaluate----------\n",
      "Distance: 0.227 / Max distance: 1.059 / Reward: 102186.671, \n",
      "Distance: 0.216 / Max distance: 1.770 / Reward: 150137.229, \n",
      "Distance: 0.219 / Max distance: 1.273 / Reward: 87446.676, \n",
      "Distance: 0.262 / Max distance: 0.874 / Reward: 168599.367, \n",
      "Distance: 0.240 / Max distance: 0.980 / Reward: 75882.682, \n",
      "Distance: 0.224 / Max distance: 0.747 / Reward: 188206.940, \n",
      "Distance: 0.221 / Max distance: 0.627 / Reward: 148117.325, \n",
      "Distance: 0.224 / Max distance: 0.791 / Reward: 107062.490, \n",
      "Distance: 0.256 / Max distance: 2.128 / Reward: 61476.024, \n",
      "Distance: 0.221 / Max distance: 0.738 / Reward: 46980.649, \n",
      "Distance: 0.205 / Max distance: 0.676 / Reward: 152895.448, \n",
      "Distance: 0.222 / Max distance: 0.992 / Reward: 127280.834, \n",
      "Distance: 0.234 / Max distance: 1.341 / Reward: 165422.904, \n",
      "Distance: 0.256 / Max distance: 0.967 / Reward: 288087.461, \n",
      "Distance: 0.206 / Max distance: 1.015 / Reward: 91702.461, \n",
      "Distance: 0.216 / Max distance: 0.825 / Reward: 191774.134, \n",
      "Distance: 0.240 / Max distance: 0.942 / Reward: 167821.212, \n",
      "Distance: 0.249 / Max distance: 0.854 / Reward: 81719.682, \n",
      "Distance: 0.234 / Max distance: 0.682 / Reward: 77579.369, \n",
      "Distance: 0.229 / Max distance: 0.969 / Reward: 76468.956, \n",
      "Distance: 0.219 / Max distance: 0.700 / Reward: 123698.041, \n",
      "Distance: 0.215 / Max distance: 0.764 / Reward: 128829.940, \n",
      "Distance: 0.246 / Max distance: 1.282 / Reward: 79068.188, \n",
      "Distance: 0.245 / Max distance: 0.666 / Reward: 65589.762, \n",
      "Distance: 0.248 / Max distance: 1.019 / Reward: 54161.360, \n",
      "Distance: 0.230 / Max distance: 1.066 / Reward: 55558.769, \n",
      "Distance: 0.214 / Max distance: 1.276 / Reward: 232749.690, \n",
      "Distance: 0.236 / Max distance: 0.809 / Reward: 231190.240, \n",
      "Distance: 0.249 / Max distance: 1.086 / Reward: 35960.461, \n",
      "Distance: 0.224 / Max distance: 0.735 / Reward: 50327.142, \n",
      "Distance: 0.219 / Max distance: 1.105 / Reward: 47074.494, \n"
     ]
    }
   ],
   "source": [
    "print(\"-----Evaluate----------\")\n",
    "# Evaluate the agent's policy once before training.\n",
    "os.system(\"rm px4_rl_output\"+str(eval_no_sample)+\".txt\")\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "os.system(\"mv px4_rl_output\"+str(eval_no_sample)+\".txt px4_rl_output\"+str(eval_no_sample)+\"_600_2.txt\")\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41df0c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.39364624023438\n"
     ]
    }
   ],
   "source": [
    "print(float(train_loss.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#620 is nan or inf error during training\n",
    "#620 failure again restored to 610\n",
    "Loss is inf or nan : Tensor had NaN values\n",
    "\t [[{{node CheckNumerics}}]] [Op:__inference_train_3398984]\n",
    "#610 failed, restored to 600\n",
    "#restarted px4 and jupyter, restore 610\n",
    "#651 drone flipped, restarted\n",
    "#652 drone flipped, restarted\n",
    "#653 drone flipped. Give up\n",
    "#repeat 600 eval to confirm statistics. stored as _600_2.txt. Original is copied at _600_0.txt\n",
    "#600 shows good eval still"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9dd44cc7cdc92411168bb684a40e881a5987d51da6be091f1590f98673f9fec9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
